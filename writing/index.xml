<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Writings on Evgenii Karimov</title><link>https://www.karimov.berlin/writing/</link><description>Recent content in Writings on Evgenii Karimov</description><generator>Hugo -- gohugo.io</generator><language>EN</language><lastBuildDate>Tue, 31 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://www.karimov.berlin/writing/index.xml" rel="self" type="application/rss+xml"/><item><title>Takeouts on new GCP products DataPlex and Analytics Hub</title><link>https://www.karimov.berlin/writing/2022-05-31-new-gcp-products/</link><pubDate>Tue, 31 May 2022 00:00:00 +0000</pubDate><guid>https://www.karimov.berlin/writing/2022-05-31-new-gcp-products/</guid><description>Google has recently introduced new products DataPlex and Analytics Hub aiming to help engineers in building a modern data engineering stack. With this writing, I&amp;rsquo;d like to share my first impressions working with them.
Let&amp;rsquo;s start with DataPlex first.
Google promotes it as a data fabric, helping to unify distributed data and automate data management by building a domain-oriented data mesh.
The idea seems to be very nice, data might be stored in multiple GCP projects and Dataplex will govern and manage it without any data movement.</description></item><item><title>Implementing slim CI for dbt with GitHub Actions</title><link>https://www.karimov.berlin/writing/2022-04-06-dbt-slim-ci/</link><pubDate>Tue, 05 Apr 2022 00:00:00 +0000</pubDate><guid>https://www.karimov.berlin/writing/2022-04-06-dbt-slim-ci/</guid><description>Recently I&amp;rsquo;ve started working with dbt, and have to say it&amp;rsquo;s quite amazing. With this post I&amp;rsquo;d like to share the approach for deploying only newly created/modified models, known as slim CI.
Imagine we have an Airflow instance executing our dbt DAG daily.
In case we introduce structural changes in our models, or simply by creating a new model, we don&amp;rsquo;t want to wait till Airflow starts on schedule. Also, we don&amp;rsquo;t want to fire the whole pipeline execution, because it might be costly to run it multiple times per day.</description></item><item><title>Thoughts about the modern data engineering stack</title><link>https://www.karimov.berlin/writing/2022-04-05-thoughts-about-data-stack/</link><pubDate>Tue, 05 Apr 2022 00:00:00 +0000</pubDate><guid>https://www.karimov.berlin/writing/2022-04-05-thoughts-about-data-stack/</guid><description>Random thoughts about the modern data engineering stack - the modern stack should be the best from the beginning, or at least aiming to be so.
It means any required vendor solutions should be onboarded as early as possible, obviously given the platform vision and financial situation at hand.
There&amp;rsquo;s no point re-inventing the wheel unless you&amp;rsquo;re trying to solve a unique use-case, which might be the truth for huge data systems like Netflix or Google has.</description></item><item><title>Reverse ETL - definition and use-cases</title><link>https://www.karimov.berlin/writing/2021-11-18-reverse-etl/</link><pubDate>Sat, 23 Oct 2021 00:00:00 +0000</pubDate><guid>https://www.karimov.berlin/writing/2021-11-18-reverse-etl/</guid><description>In this article I&amp;rsquo;d like to summarise my knowledges and understanding of the new buzzword Reverse ETL.
No long introductions, let&amp;rsquo;s jump into the topic.
Let&amp;rsquo;s start with definitions, what is ETL and Reverse ETL?
ETL (Extract-Transform-Load) - a common term for data integration, a process of bringing data from sources to your datawarehouse, or data lake.
Modern data storage and analytics vendors also recommend utilising ELT approach, when transformations and aggregations happen after the load into the datastore.</description></item><item><title>DWH approaches</title><link>https://www.karimov.berlin/writing/2021-04-30-dwh-structure-approaches/</link><pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate><guid>https://www.karimov.berlin/writing/2021-04-30-dwh-structure-approaches/</guid><description>With this post I&amp;rsquo;d like to cover some well-know approaches to data-warehousing in order to have a brief overview of them:
Inmon Kimball Data Vault Data Lake Lakehouse I&amp;rsquo;m not aiming to cover all the details of each approach in this article, as each of them deserves its own separate article.
Inmon Let&amp;rsquo;s start first with the methodology created by Bill Inmon, which is considered to be a top-down approach.</description></item><item><title>Connecting people</title><link>https://www.karimov.berlin/writing/2021-02-25-clubhouse/</link><pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate><guid>https://www.karimov.berlin/writing/2021-02-25-clubhouse/</guid><description>An application which is not available to everyone helps those, who have it, to make a secret out of what was democratized and devalued to the limit - own talk-shit. For this purpose, nothing is written or saved there. What people kept as the highest value - knowledge - is now in such a wild overproduction that potlatch is needed, ritual destruction.
Like an avid alcoholic, the golden billion stops for a minute to vomit unnecessary information, and, wiping its mouth, again lies down at the table, where continues to feed each other.</description></item><item><title>About the modern data engineering stack</title><link>https://www.karimov.berlin/writing/2021-02-22-modern-data-eng-stack/</link><pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate><guid>https://www.karimov.berlin/writing/2021-02-22-modern-data-eng-stack/</guid><description>Not a secret we&amp;rsquo;re living in the rapidly changing world, generating more and more machine data. A new era of big data and so on, words you&amp;rsquo;ve probably heard hundreds of times already. A lot of new and old companies want to get insights out of their data. Become data-driven is a new trend and motto. Naturally many new, open-source tools and projects appeared against this background.
How can they help in democratising data landscape and how does the choice between commercial products/services and open-source/in-house solutions look like in these circumstances?</description></item><item><title>Notes about data catalog solutions</title><link>https://www.karimov.berlin/writing/2021-01-25-data-catalog/</link><pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate><guid>https://www.karimov.berlin/writing/2021-01-25-data-catalog/</guid><description>Data catalogs have become an essential part of the modern data infrastructure and management. It allows to have an overview of data, presented in the organisation, by storing metadata about it - such as location, format, columns/attributes, etc. This is especially important combined with the modern data teams architectures, such as Data Mesh, when every team can contribute to data catalog.
There&amp;rsquo;s a growing interest in the industry to improve productivity of data engineers and scientists with metadata.</description></item><item><title>Choosing open-source comment system</title><link>https://www.karimov.berlin/writing/2021-01-11-comments-system/</link><pubDate>Mon, 11 Jan 2021 00:00:00 +0000</pubDate><guid>https://www.karimov.berlin/writing/2021-01-11-comments-system/</guid><description>During this web-site development I naturally had an idea to add a comment system here. Although Hugo, which I use, supports Disqus out of the box, I&amp;rsquo;m not so happy about it due to quite controversial opinions about it.
So I started to search for alternatives and stopped on Commento - it looks quite lightweight and easy to setup.
So I set all up, added JS/HTML things, and wanted to enjoy - but an issue is apparently appeared when I tried to connect 3-rd party auth providers, particularly Github.</description></item><item><title>Switching to Notion after many years of using Onenote</title><link>https://www.karimov.berlin/writing/2020-12-16-notion-vs-onenote/</link><pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate><guid>https://www.karimov.berlin/writing/2020-12-16-notion-vs-onenote/</guid><description>OneNote was my primary place for collecting every note for many years. I started to use it long ago, maybe 10 years ago in the times when Windows phones still exist. It suited my needs for many years, but while reading the article about &amp;ldquo;Zettelkasten&amp;rdquo; notes method (highly recommend to have a look!) I found that probably I need to review my tools and try something new allowing to build links between my notes.</description></item><item><title>Some lessons learned working with Spark/AWS Glue</title><link>https://www.karimov.berlin/writing/2020-11-23-lessons-learned-spark-glue/</link><pubDate>Mon, 23 Nov 2020 00:00:00 +0000</pubDate><guid>https://www.karimov.berlin/writing/2020-11-23-lessons-learned-spark-glue/</guid><description>Random general and specific notes from Notion made after a couple of projects working with Spark/AWS Glue:
Parquet format doesn&amp;rsquo;t support empty arrays Schema evolution in Spark 2.4 for ORC format is broken Looking forward for ZSTD compression for ORC format. Always check that desired features work as expected - apparently dynamic partitioning pruning doesn&amp;rsquo;t work in AWS Glue as expected with broadcast join, reading the whole dataset on the left side.</description></item></channel></rss>