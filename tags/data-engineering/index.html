<html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Evgenii Karimov - data engineering</title><meta name=description content="Personal homepage"><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet async href=/fontawesome-free-5.12.1-web/css/all.css><script src=/self/js/lazysizes.min.js async></script><link rel=apple-touch-icon sizes=180x180 href="/self/img/icons/favicon/apple-touch-icon.png?v=2"><link rel=icon type=image/png sizes=32x32 href="/self/img/icons/favicon/favicon-32x32.png?v=2"><link rel=icon type=image/png sizes=16x16 href="/self/img/icons/favicon/favicon-16x16.png?v=2"><link rel=manifest href="/self/img/icons/favicon/site.webmanifest?v=2"><link rel=stylesheet href=https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css integrity=sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.4.1.slim.min.js integrity=sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js integrity=sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo crossorigin=anonymous></script><script src=https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js integrity=sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6 crossorigin=anonymous></script><link href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Montserrat:400,700" rel=stylesheet><link rel=stylesheet async href=/self/css/custom.css><script>var remark_config={host:"https:\/\/remark42.karimov.berlin",site_id:'www.karimov.berlin',components:['embed'],max_shown_comments:20,theme:'light',locale:'EN',show_email_subscription:false};(function(c){for(var i=0;i<c.length;i++){var d=document,s=d.createElement('script');s.src=remark_config.host+'/web/'+c[i]+'.js';s.defer=true;(d.head||d.body).appendChild(s);}})(remark_config.components||['embed']);</script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-73265627-3','auto');ga('send','pageview');}</script><script src=https://code.jquery.com/jquery-1.12.4.min.js integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin=anonymous></script><script src=/self/js/load-photoswipe.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.css integrity="sha256-sCl5PUOGMLfFYctzDW3MtRib0ctyUvI9Qsmq2wXOeBY=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/default-skin/default-skin.min.css integrity="sha256-BFeI1V+Vh1Rk37wswuOYn5lsTcaU96hGaI7OUVCLjPc=" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.js integrity="sha256-UplRCs9v4KXVJvVY+p+RSo5Q4ilAUXh7kpjyIP5odyc=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe-ui-default.min.js integrity="sha256-PWHOlUzc96pMc8ThwRIXPn8yH4NOLu42RQ0b9SpnpFk=" crossorigin=anonymous></script><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div></head><body><nav class="navbar navbar-expand-sm navbar-light font-weight-bold border-bottom"><div class=navbar-header><a class=navbar-brand href=https://www.karimov.berlin/en>Evgenii Karimov</a></div><button class=navbar-toggler type=button data-toggle=collapse data-target=#collapsibleNavbar>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=collapsibleNavbar><ul class="navbar-nav ml-auto"><li class="nav-item active"><a class="nav-link navbar-dark" href=https://www.karimov.berlin/writing/>Writing</a></li><li class="nav-item active"><a class="nav-link navbar-dark" href=https://www.karimov.berlin/projects/>Projects</a></li><li class="nav-item active"><a class="nav-link navbar-dark" href=https://www.karimov.berlin/photos/>Photos</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" id=dropdownMenuButton role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><i class="fas fa-globe"></i>EN</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=dropdownMenuButton><a class=dropdown-item><i class="fas fa-flag-en"></i><b>English</b></a>
<a class=dropdown-item href=https://www.karimov.berlin/de/tags/data-engineering/><i class="fas fa-flag-de"></i>Deutsch</a>
<a class=dropdown-item href=https://www.karimov.berlin/ru/tags/data-engineering/><i class="fas fa-flag-ru"></i>Русский</a></div></li></ul></div></nav><div class="jumbotron header-font border"><div class=text-center><a href=https://www.karimov.berlin/en><img class="lazyload rounded-circle rounded-50 border-white bg-white mx-auto avatar-100" data-src=https://www.karimov.berlin/self/img/avatar.jpg></a>
<a href=https://www.karimov.berlin/en><h1 class="text-dark font-weight-bold">Evgenii Karimov</h1></a><h2>Developer, data engineer<br>Blog, photos and more</h2></div></div>Thoughts about the modern data engineering stack<p>Random thoughts about the modern data engineering stack - the modern stack should be the best from the beginning,
or at least aiming to be so.</p><p>It means any required vendor solutions should be onboarded as early as possible,
obviously given the platform vision and financial situation at hand.</p><p>There&rsquo;s no point re-inventing the wheel unless you&rsquo;re trying to solve a unique use-case,
which might be the truth for huge data systems like Netflix or Google has.
Migrations in data world are much more complex comparing it to the regular database migrations in software engineering,
because of the eco-system around all components.
It&rsquo;s hard to isolate and replace a single component.</p><p>Today, when so many products ease the life of a data engineer (managed ETL, data cataloging, governance, etc),
it&rsquo;s already a challenge to pick right and integrate them with each other,
making the job of data analysts and scientists intuitive and easy-going.</p>Reverse ETL - definition and use-cases<p>In this article I&rsquo;d like to summarise my knowledges and understanding of the new buzzword Reverse ETL.</p><p>No long introductions, let&rsquo;s jump into the topic.</p><p>Let&rsquo;s start with definitions, what is ETL and Reverse ETL?</p><p>ETL (Extract-Transform-Load) - a common term for data integration, a process of bringing data from sources to your datawarehouse, or data lake.</p><p><img src=/self/img/2021-11-18-reverse-etl/etl.jpg alt></p><p>Modern data storage and analytics vendors also recommend utilising ELT approach, when transformations and aggregations happen after the load into the datastore.</p><p><img src=/self/img/2021-11-18-reverse-etl/elt.jpg alt></p><p>So what is reverse ETL? As you could guess out of the name - it turns the story the other way around, i.e. it&rsquo;s the process of moving data from the central datastore to operational systems.</p><p><img src=/self/img/2021-11-18-reverse-etl/reverse-etl.jpg alt></p><p>Just google for &ldquo;Reverse ETL&rdquo; to find different vendors pitching their solutions. Most of them would be relatively young, during my research I&rsquo;ve found Hightouch, Census, Grouparoo, Rudderstack, Rivery.</p><p>Use-cases examples for reverse ETL:</p><p>- Identify customer at risk, to prevent customer churn before it happens</p><p>- Drive new sales by matching data from CRM and other sources</p><p>- Data replication to cloud apps for finding new insights and operational analytics</p><p>A regular task for any data engineer is writing a new ETL connector for a new source (or using special tools to ease the process). But having an ETL connector doesn&rsquo;t mean that a reverse ETL connector is also available. In fact, the most common case would be the need to build a reserve ETL connector separately, and to take care about API endpoints, rate limits, etc.</p><p>Real-time scenario requirements:</p><p>In cases when real-time data analysis gives significant benefits (in my personal opinion it&rsquo;s not always the case), it might be complicated to combine it with a reverse ETL - imagine you have hundreds of thousands events per minute, it&rsquo;d be difficult to process all the events through a datawarehouse, designed primarly for batch and complex analytical queries. So in this case it probably makes sense to utilise a regular streaming solutions, like Kafka, optionally with KSQL, and stream necessary data into it, combining in together within the stream itself.</p><p>I personally find the concept of reverse ETL very inspiring, transforming a traditional DWH or data lake into a source of truth across all the systems and 3-rd party providers used inside the company.</p><p>Exciting times we&rsquo;re living in!</p>DWH approaches<p>With this post I&rsquo;d like to cover some well-know approaches to data-warehousing in order to have a brief overview of them:</p><ul><li><a href=#Inmon>Inmon</a></li><li><a href=#Kimball>Kimball</a></li><li><a href=#Data-Vault>Data Vault</a></li><li><a href=#Data-Lake>Data Lake</a></li><li><a href=#Lakehouse>Lakehouse</a></li></ul><p>I&rsquo;m not aiming to cover all the details of each approach in this article, as each of them deserves its own separate article.</p><h3 id=inmon>Inmon</h3><p>Let&rsquo;s start first with the methodology created by Bill Inmon, which is considered to be a top-down approach.
This methodology assumes that the structure of the data is known from top to down, there&rsquo;s a whole picture of it availble
and data is modelled accordingly to the 3NF (Normal form).
Each logical model contains all the details related to that entity. The key point is that data stored in the normalized form.
Data redundancy is avoided as much as possible.
The biggest downside of this approach is that the data should be described from top to down, what required very skillful people
to design it and keep integration between all the components, which might be complicated in the enterprise-level companies.</p><h3 id=kimball>Kimball</h3><p>In opposite to the Inmon approach this one by Ralph Kimball is considered to be a bottom-up approach.
Key sources (operational systems) are identified and documented.<br>Normally it&rsquo;s easier to star twith Kimball, to split the data into datamarts accroding to each department. But it&rsquo;s harder to scale it later.<br>ETL tools bring data from different sources and load it into the staging area.
Then data is loaded into the dimensional tables. The key point is that the data isn&rsquo;t normalized as data is organized
either in Star or Snowflake (under certain circumstances) schema.</p><h3 id=data-vault>Data Vault</h3><p>The DV model, in a nutshell, is a layer that exists between regular dimensional modeling (OLAP, Star Schema) and
Staging that provides scaling with growing business requirements and serves to break down complexities of both the modeling and ETL.
It’s composed of hubs (business entities), links (relationships) and satellites (descriptive attributes)
It&rsquo;s especially useful, when a good support of tracking historical changes is required, as it stores all the facts,
without distinction to good and bad data, is opposite to other DWH methods of storing &ldquo;a single version of truth&rdquo;.</p><p>The modeling method is designed to be resilient to change in the business environment
where the data being stored is coming from, by explicitly separating structural information from descriptive attributes.
Data vault is designed to enable parallel loading as much as possible, so that very large implementations can
scale out without the need for major redesign.</p><h3 id=data-lake>Data lake</h3><p>A very popular approach nowadays, as amount of data grows significantly and introduces silos between different departments,
especially in big enterprise companies.
Opposite to the traditional DWH approaches, Data Lake stores all the information from source systems as-is (i.e. raw data),
optionally some auditing columns to show where the data came from, when it was loaded, etc.
Important to mention it&rsquo;s not just data is use today, but also data that may be used and even data that may never be used just
because it might be used someday.
Hence, commodity, off-the-shelf servers combined with cheap storage makes scaling a data lake to terabytes and petabytes fairly reasonable.
It&rsquo;s like a giant tub of assorted Lego bricks and no defined plan as to how to put it together,
and some of the bricks will be non-standard – it’s up to the person playing with the bricks to assemble it how they see fit to meet their needs.</p><p>Some data lakes have multiple layers e.g. the raw data layer and then a governed data layer where the data has been cleansed,
standardised, etc. - but is still in basically the same structure as in the raw data layer.</p><h3 id=lakehouse>Lakehouse</h3><p>This approach tends to take the best from both worlds: Data Warehouse and Data Lake, providing a single platform
for all data transformations, business intelligence and data science.
Storage and compute levels are separated, implementing similar data structures and data managements features to those in a data darehouse,
directly on the kind of low-cost storage used for data lakes.
The approach assumes the following advantages:</p><ul><li>Data governance may be simplified with a single control point</li><li>Keep data in the same data lake format</li><li>Schema management is simplified</li><li>Transaction support with ACID compliance</li></ul>About the modern data engineering stack<p>Not a secret we&rsquo;re living in the rapidly changing world, generating more and more machine data.
A new era of big data and so on, words you&rsquo;ve probably heard hundreds of times already.
A lot of new and old companies want to get insights out of their data. Become data-driven is a new trend and motto.
Naturally many new, open-source tools and projects appeared against this background.</p><p>How can they help in democratising data landscape and how does the choice between commercial products/services
and open-source/in-house solutions look like in these circumstances?</p><p>I won&rsquo;t give a definitive answer, but rather share my thoughts.</p><p>Every case is pretty unique, although every company can experience similar issues/need of similar instruments.</p><p>Almost every single job description contains some of these keywords: Airflow, Python/SQL, Redshift/Snowflake/BigQuery, Tableau/Looker.</p><p>While it&rsquo;s totally understandable why the latter two are in this list,
I&rsquo;m often curious if it&rsquo;s absolutely necessary to write and maintain a bunch of operators/dags/hooks for Airflow?</p><p>Please get me right, I truly love Airflow, it&rsquo;s a super nice tool,
but still - can data engineers concentrate on more interesting tasks, rather than writing another connector to another source API?
In this light the modern instruments like <a href=https://fivetran.com/>Fivetran</a>/<a href=https://www.getdbt.com/>dbt</a> look quite interesting.</p><p>So here we come to touch the topic of when it makes sense to have a team of data engineers,
maintaining all the DAGs/pipes copying data from sources and custom operators/logic.
It makes sense in the following cases:</p><ul><li>Amount of sources are low, and they aren&rsquo;t frequently added/changed</li><li>Special security concerns</li><li>Specific data sources, which aren&rsquo;t supported by cloud pipelines.</li></ul><p>Every other use-case will potentially win by using <a href=https://www.stitchdata.com/>Stitch</a>/Fivetran/etc - simply because it&rsquo;d be more reliable solution.</p><p>Anyways it&rsquo;s still possible to combine the best from both worlds and use as Airflow tasks, as one of cloud pipelines to ingest the data.
Also there shouldn&rsquo;t be any fear regarding the data engineer&rsquo;s role - no one takes the job position away, but making it easier.
I personally don&rsquo;t want to make myself an important employee, just because I know how a complex piece of some data fetcher&rsquo;s code works.</p><p>In the same time, there&rsquo;s another important factor - team&rsquo;s professional level.
I&rsquo;d agree, that teams having it strong, would potentially benefit more by using their own solutions.
Simply because it&rsquo;d be enough flexible and tuned exactly to fit the company needs.
Quite often such teams also contribute back to the open-source community.</p><p>This is especially actual in the light of the modern data teams concepts - like <a href=https://martinfowler.com/articles/data-monolith-to-mesh.html>Data Mesh</a>,
when the bottle-neck layer of data engineers, responsible for every single data load,
is pushed back to business-teams using this data,
and ideally creating and maintaining ingestion pipelines with the help of data engineers, who support self-service data platform.</p><p>Thank you so much if you read the article all the way here.
I’d love to hear about your opinion and experiences.</p><footer class="footer card-footer text-secondary mt-auto"><div class=row><div class="col col-sm-9">&copy; 2022 Evgenii Karimov</div><div class="col-12 col-sm-3 text-sm-right px-3"><a href=mailto:eskarimov@gmail.com class="text-nowrap text-secondary"><i class="fas fa-envelope fa-fw valign-middle" data-hint=Email></i></a><a href=https://www.linkedin.com/in/evgenii-karimov/ class="text-nowrap text-secondary"><i class="fab fa-linkedin-in fa-fw valign-middle" data-hint=LinkedIn></i></a><a href=https://github.com/eskarimov class="text-nowrap text-secondary"><i class="fab fa-github fa-fw valign-middle" data-hint=Github></i></a><a href=https://www.instagram.com/eskarimov class="text-nowrap text-secondary"><i class="fab fa-instagram fa-fw valign-middle" data-hint=Instagram></i></a></div></div></footer></body></html>