<html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Evgenii Karimov - data engineering</title><meta name=description content="Personal homepage"><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet async href=/fontawesome-free-5.12.1-web/css/all.css><script src=/self/js/lazysizes.min.js async></script><link rel=apple-touch-icon sizes=180x180 href="/self/img/icons/favicon/apple-touch-icon.png?v=2"><link rel=icon type=image/png sizes=32x32 href="/self/img/icons/favicon/favicon-32x32.png?v=2"><link rel=icon type=image/png sizes=16x16 href="/self/img/icons/favicon/favicon-16x16.png?v=2"><link rel=manifest href="/self/img/icons/favicon/site.webmanifest?v=2"><link rel=stylesheet href=https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css integrity=sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.4.1.slim.min.js integrity=sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js integrity=sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo crossorigin=anonymous></script><script src=https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js integrity=sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6 crossorigin=anonymous></script><link href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Montserrat:400,700" rel=stylesheet><link rel=stylesheet async href=/self/css/custom.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-73265627-3','auto');ga('send','pageview');}</script><script src=https://code.jquery.com/jquery-1.12.4.min.js integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin=anonymous></script><script src=/self/js/load-photoswipe.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.css integrity="sha256-sCl5PUOGMLfFYctzDW3MtRib0ctyUvI9Qsmq2wXOeBY=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/default-skin/default-skin.min.css integrity="sha256-BFeI1V+Vh1Rk37wswuOYn5lsTcaU96hGaI7OUVCLjPc=" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.js integrity="sha256-UplRCs9v4KXVJvVY+p+RSo5Q4ilAUXh7kpjyIP5odyc=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe-ui-default.min.js integrity="sha256-PWHOlUzc96pMc8ThwRIXPn8yH4NOLu42RQ0b9SpnpFk=" crossorigin=anonymous></script><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div></head><body><nav class="navbar navbar-expand-sm navbar-light font-weight-bold border-bottom"><div class=navbar-header><a class=navbar-brand href=https://www.karimov.berlin/en>Evgenii Karimov</a></div><button class=navbar-toggler type=button data-toggle=collapse data-target=#collapsibleNavbar>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=collapsibleNavbar><ul class="navbar-nav ml-auto"><li class="nav-item active"><a class="nav-link navbar-dark" href=https://www.karimov.berlin/writing/>Writing</a></li><li class="nav-item active"><a class="nav-link navbar-dark" href=https://www.karimov.berlin/projects/>Projects</a></li><li class="nav-item active"><a class="nav-link navbar-dark" href=https://www.karimov.berlin/photos/>Photos</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" id=dropdownMenuButton role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><i class="fas fa-globe"></i>EN</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=dropdownMenuButton><a class=dropdown-item><i class="fas fa-flag-en"></i><b>English</b></a>
<a class=dropdown-item href=https://www.karimov.berlin/de/tags/data-engineering/><i class="fas fa-flag-de"></i>Deutsch</a>
<a class=dropdown-item href=https://www.karimov.berlin/ru/tags/data-engineering/><i class="fas fa-flag-ru"></i>Русский</a></div></li></ul></div></nav><div class="jumbotron header-font border"><div class=text-center><a href=https://www.karimov.berlin/en><img class="lazyload rounded-circle rounded-50 border-white bg-white mx-auto avatar-100" data-src=https://www.karimov.berlin/self/img/avatar.jpg></a>
<a href=https://www.karimov.berlin/en><h1 class="text-dark font-weight-bold">Evgenii Karimov</h1></a><h2>Developer, data engineer<br>Blog, photos and more</h2></div></div>Combining dbt and SQLFluff<p>In this post I&rsquo;d like to share how to adopt two very useful tools <a href=https://www.sqlfluff.com/>SQLFluff</a>, a SQL linter,
and <a href=https://www.getdbt.com/>dbt</a>, a data modelling tool, into a powerful combination and make it work with pre-commit to ensure all your
SQLs are properly formatted.</p><p>Why to combine them at all?
There are a lot of ways to write SQL: leading/trailing commas, lower-upper case of keywords,
aliasing table and column names, etc.</p><p>Since every data engineer and analyst spends a lot of time reading SQL code,
having a standard style helps to improve this process and also makes writing less error-prone.</p><p>dbt is a powerful data modelling tool helping to write less boilerplate code with Jinja templates and
it has a remarkable ecosystem of different plugins.</p><p>Let&rsquo;s have a look at the example, say we have the following SQL:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=color:#66d9ef>SELECT</span>
  order_id,
  ORDER_DATE <span style=color:#66d9ef>AS</span> order_date
  , order_price <span style=color:#f92672>*</span><span style=color:#ae81ff>0</span>.<span style=color:#ae81ff>9</span> <span style=color:#66d9ef>AS</span> discounted_price
<span style=color:#66d9ef>from</span> orders
</code></pre></div><p>Doesn&rsquo;t look very reader-friendly, does it?<br>Now I&rsquo;m going to install sqlfluff with pip and create a configuration file for it.<br>In this example I&rsquo;ll be using BigQuery as a query engine, so you might need to adjust my code for your setup,
as well as to enforce certain rules (<a href=https://docs.sqlfluff.com/en/stable/rules.html>reference</a>):</p><p>Installing sqlfluff, dbt and dependent packages:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>pip install sqlfluff
pip install dbt-core
pip install dbt-bigquery
pip install sqlfluff-templater-dbt
</code></pre></div><p>How my <code>sqlfliff</code> looks like:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=color:#66d9ef>[sqlfluff]</span>
<span style=color:#a6e22e>verbose</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>1</span>
<span style=color:#a6e22e>dialect</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>bigquery</span>
<span style=color:#a6e22e>templater</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>dbt</span>
<span style=color:#a6e22e>recurse</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>0</span>
<span style=color:#a6e22e>runaway_limit</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>10</span>
<span style=color:#a6e22e>ignore_templated_areas</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>True</span>
<span style=color:#a6e22e>encoding</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>autodetect</span>
<span style=color:#a6e22e>processes</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>4</span>

<span style=color:#75715e># NB: This config will only apply in the root folder.</span>
<span style=color:#a6e22e>sql_file_exts</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>.sql
</span><span style=color:#e6db74>    </span>
<span style=color:#66d9ef>[sqlfluff:indentation]</span>
<span style=color:#a6e22e>indented_joins</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>False</span>
<span style=color:#a6e22e>indented_using_on</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>False</span>
<span style=color:#a6e22e>template_blocks_indent</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>True
</span><span style=color:#e6db74>    </span>
<span style=color:#66d9ef>[sqlfluff:templater]</span>
<span style=color:#a6e22e>unwrap_wrapped_queries</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>True
</span><span style=color:#e6db74>    </span>
<span style=color:#66d9ef>[sqlfluff:templater:dbt]</span>
<span style=color:#a6e22e>profiles_dir</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>profiles/</span>
<span style=color:#a6e22e>profile</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>bigquery</span>
<span style=color:#a6e22e>target</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>prod
</span><span style=color:#e6db74>    </span>
<span style=color:#66d9ef>[sqlfluff:templater:jinja]</span>
<span style=color:#a6e22e>apply_dbt_builtins</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>True
</span><span style=color:#e6db74>    </span>
<span style=color:#66d9ef>[sqlfluff:rules]</span>
<span style=color:#a6e22e>tab_space_size</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>4</span>
<span style=color:#a6e22e>max_line_length</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>110</span>
<span style=color:#a6e22e>indent_unit</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>space</span>
<span style=color:#a6e22e>comma_style</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>leading</span>
<span style=color:#a6e22e>allow_scalar</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>True</span>
<span style=color:#a6e22e>single_table_references</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>consistent</span>
<span style=color:#a6e22e>unquoted_identifiers_policy</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>all</span>
</code></pre></div><p>Also we need to ignore certain dbt folders by adding them into <code>.sqlfluffignore</code>:</p><pre><code>target/
dbt_modules/
logs/
</code></pre><p>After running <code>sqlfluff fix</code> we get:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=color:#66d9ef>SELECT</span>
    order_id
    , order_date <span style=color:#66d9ef>AS</span> order_date
    , order_price <span style=color:#f92672>*</span> <span style=color:#ae81ff>0</span>.<span style=color:#ae81ff>9</span> <span style=color:#66d9ef>AS</span> discounted_price
<span style=color:#66d9ef>FROM</span> orders
</code></pre></div><p>Integration with pre-commit hooks works following:</p><ul><li><a href=https://pre-commit.com/>Setup pre-commit</a> for your repo.</li><li>Add the hooks into <code>.pre-commit-config.yaml</code> as follows:</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>  - <span style=color:#f92672>repo</span>: <span style=color:#ae81ff>https://github.com/sqlfluff/sqlfluff</span>
    <span style=color:#f92672>rev</span>: <span style=color:#ae81ff>1.1.0</span>
    <span style=color:#f92672>hooks</span>:
      - <span style=color:#f92672>id</span>: <span style=color:#ae81ff>sqlfluff-fix</span>
        <span style=color:#f92672>additional_dependencies</span>: [<span style=color:#e6db74>&#39;dbt-core==1.1.0&#39;</span>, <span style=color:#e6db74>&#39;dbt-bigquery==1.1.0&#39;</span>, <span style=color:#e6db74>&#39;sqlfluff-templater-dbt==1.1.0&#39;</span>]
        <span style=color:#f92672>files</span>: |<span style=color:#e6db74>
</span><span style=color:#e6db74>          models/|
</span><span style=color:#e6db74>          tests/</span>          
      - <span style=color:#f92672>id</span>: <span style=color:#ae81ff>sqlfluff-lint</span>
        <span style=color:#f92672>additional_dependencies</span>: [<span style=color:#e6db74>&#39;dbt-core==1.1.0&#39;</span>, <span style=color:#e6db74>&#39;dbt-bigquery==1.1.0&#39;</span>, <span style=color:#e6db74>&#39;sqlfluff-templater-dbt==1.1.0&#39;</span>]
        <span style=color:#f92672>files</span>: |<span style=color:#e6db74>
</span><span style=color:#e6db74>          models/|
</span><span style=color:#e6db74>          tests/</span>          
</code></pre></div><p>Enjoy!<br>Now everytime you want to commit a new change in your dbt models, sqlfluff will try to fix it automatically for you.<br>Additionally you can integrate this check with Github Actions or other CI/CD tools and require <code>sqlfluff lint</code> to pass
before making merging a new code into the main branch.</p>Takeouts on new GCP products DataPlex and Analytics Hub<p>Google has recently introduced new products <a href=https://cloud.google.com/blog/products/data-analytics/build-a-data-mesh-on-google-cloud-with-dataplex-now-generally-available>DataPlex</a>
and <a href=https://cloud.google.com/blog/products/data-analytics/analytics-hub-data-exchange-now-in-public-preview>Analytics Hub</a> aiming to help engineers in building a modern data engineering stack.
With this writing, I&rsquo;d like to share my first impressions working with them.</p><p>Let&rsquo;s start with DataPlex first.</p><p>Google promotes it as a data fabric, helping to unify distributed data and automate data management by building a domain-oriented data mesh.</p><p>The idea seems to be very nice, data might be stored in multiple GCP projects and Dataplex will govern and manage it without any data movement.</p><p>Let&rsquo;s dive deeper and see how it&rsquo;s implemented.</p><p>First, we need to create a new lake. Additionally, we can specify if we want to enable Hive Metastore service useful for querying data with instruments such as Presto, Hive, Spark.</p><p><img src=/self/img/2022-05-31-new-gcp-products/create_new_lake.jpg alt></p><p>On the next step we should get familiar with a concept of zones inside our lake.
A zone is a logical group of structured or unstructured data, such as GCS buckets and/or BQ datasets and tables.</p><p>There are 2 zones types available: raw and curated. Raw stores data in any format in GCS or BQ datasets. A curated zone stores structured data in GCS or BQ in certain formats such as Parquet, Avro or ORC.</p><p>So let&rsquo;s see how it works and create one raw and one curated zone.</p><p><img src=/self/img/2022-05-31-new-gcp-products/create_zones.jpg alt>
This will accordingly create 2 datasets in BigQuery: analytics_raw, analytics_curated.</p><p>And now we come to the point where I personally get confused about Dataplex&rsquo;s concept.
I assumed that adding a new table to these datasets will make this table visible in Dataplex &ldquo;Discover&rdquo; or &ldquo;Explore&rdquo; tab.
However, when I added a new table to the raw dataset, nothing has appeared here.</p><p><img src=/self/img/2022-05-31-new-gcp-products/example_table_analytics_raw.jpg alt>
A new table is added to the <code>analytics_raw</code> dataset.</p><p><img src=/self/img/2022-05-31-new-gcp-products/analytics_raw_assets.jpg alt>
However, no assets are shown under <code>analytics-raw</code> and nothing is displayed in catalog either.</p><p>I could go even further and delete the dataset completely from BQ, and it won&rsquo;t affect the zone in Dataplex anyhow. So, I&rsquo;m not sure what&rsquo;s the right relation between a zone in Dataplex and an according dataset in BQ.</p><p>Let&rsquo;s move on and try adding an asset to our zones.<br>Assets help to map data stored in GCS or BQ into a single zone within a lake.</p><p>Here I&rsquo;m adding a dataset <code>marts</code> containing a table <code>currency_rates</code> into the curated zone.
And here, suddenly, I can explore and find this dataset and table within &ldquo;Explore&rdquo; tab in Dataplex.</p><p>It&rsquo;s a mystery to me why the same doesn&rsquo;t work with datasets created per each zone.</p><p><img src=/self/img/2022-05-31-new-gcp-products/curated_zone_asset.jpg alt></p><p>Let&rsquo;s stop here for reviewing Analytics Hub and get back to Dataplex right after it, discussing which features from Analytics Hub might be very useful in Dataplex.</p><p>Analytics Hub is a platform for exchanging data across organization and external data providers.<br>It is based on publish and subscribe model of BigQuery datasets.</p><p>In simple words, you can create a link to a dataset in another project inside yours, allowing to view and query it.<br>Of course, it&rsquo;s not a big difference comparing it to regular grants to users, groups, service accounts or authorized views, however, having a linked dataset in your project makes is great for discovery and access control.</p><p>Here&rsquo;s how it works - there&rsquo;s a new tab in BQ menu, where you can find data exchanges and browse Analytics Hub datasets.</p><p><img src=/self/img/2022-05-31-new-gcp-products/analytics_hub.jpg alt></p><p>Important to mention, that you need to have enough a role Analytics Hub Viewer to a listing you&rsquo;re interested, in order to discover it in Analytics Hub catalog.<br>After you&rsquo;ve found an interesting dataset, you can &ldquo;subscribe&rdquo; to it, then a new linked dataset will appear within your project, which will be a read-mirror of the original dataset.</p><p>In my opinion it&rsquo;s a great improvement, however, it&rsquo;s lacking functionality to preview data (before having access to it) and requesting data owner to give you permissions.</p><p>What&rsquo;s missing to me overall is the information how to combine all the new products together - Dataplex, Analytics Hub, Data Catalog, plus now we have BigTable.
Google provides mainly marketing materials and videos, while in reality many questions or best practices left unanswered.</p><p>Hopefully these products will evolve together into a well-combined environment. Say, I&rsquo;d really want to have an ability of managing access to multiple datasets within a single place, so that anyone within the company could preview the data and request access to it, and then a granted dataset would appear as a link in the requester&rsquo;s project.</p><p>Have you tried any of these new products or have any other thoughts? Please welcome to comments :)</p>Implementing slim CI for dbt with GitHub Actions<p>Recently I&rsquo;ve started working with dbt, and have to say it&rsquo;s quite amazing.
With this post I&rsquo;d like to share the approach for deploying only newly created/modified models, known as slim CI.</p><p>Imagine we have an Airflow instance executing our dbt DAG daily.</p><p>In case we introduce structural changes in our models, or simply by creating a new model,
we don&rsquo;t want to wait till Airflow starts on schedule.
Also, we don&rsquo;t want to fire the whole pipeline execution, because it might be costly to run it multiple times per day.</p><p>What we could do in this case is to introduce a new CI step,
which will compare differences between old and new models,
and run only those, which have changed (and their dependencies/downstream tasks).</p><p>How it works with GitHub Actions:</p><p>say we have <code>deploy_dbt.yml</code> file describing our CI actions</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yml data-lang=yml><span style=color:#f92672>name</span>: <span style=color:#e6db74>&#39;Deploy dbt&#39;</span>

<span style=color:#f92672>on</span>:
  <span style=color:#f92672>push</span>:
    <span style=color:#f92672>branches</span>:
      - <span style=color:#ae81ff>main</span>
<span style=color:#f92672>env</span>:
  <span style=color:#f92672>DBT_VERSION</span>: <span style=color:#ae81ff>1.0.0</span>
<span style=color:#f92672>jobs</span>:
  <span style=color:#f92672>dbt-slim-ci</span>:
    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Deploy dbt models</span>
    <span style=color:#f92672>runs-on</span>: <span style=color:#ae81ff>ubuntu-latest</span>
    <span style=color:#f92672>permissions</span>:
      <span style=color:#f92672>actions</span>: <span style=color:#ae81ff>read</span>
      <span style=color:#f92672>contents</span>: <span style=color:#ae81ff>read</span>
    <span style=color:#f92672>steps</span>:
      - <span style=color:#f92672>uses</span>: <span style=color:#ae81ff>actions/checkout@v2</span>
      <span style=color:#75715e># Get commit hash of the last successful deployment, it will be used for comparison</span>
      - <span style=color:#f92672>uses</span>: <span style=color:#ae81ff>nrwl/last-successful-commit-action@v1</span>
        <span style=color:#f92672>id</span>: <span style=color:#ae81ff>last_successful_commit</span>
        <span style=color:#f92672>with</span>:
          <span style=color:#f92672>branch</span>: <span style=color:#e6db74>&#39;main&#39;</span>
          <span style=color:#f92672>workflow_id</span>: <span style=color:#e6db74>&#39;deploy_dbt.yml&#39;</span>
          <span style=color:#f92672>github_token</span>: <span style=color:#ae81ff>${{ secrets.GITHUB_TOKEN }}</span>
      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Checkout last successful main</span>
        <span style=color:#f92672>uses</span>: <span style=color:#ae81ff>actions/checkout@v2</span>
        <span style=color:#f92672>with</span>:
          <span style=color:#f92672>ref</span>: <span style=color:#ae81ff>${{ steps.last_successful_commit.outputs.commit_hash }}</span>
          <span style=color:#f92672>path</span>: <span style=color:#ae81ff>last_successful_main/</span>
      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Install dbt &amp; packages</span>
        <span style=color:#f92672>run</span>: |-<span style=color:#e6db74>
</span><span style=color:#e6db74>          pip3 install dbt==${{ env.DBT_VERSION }}
</span><span style=color:#e6db74>          dbt deps</span>          
      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Generate last successful manifest.json</span>
        <span style=color:#f92672>run</span>: &gt;<span style=color:#e6db74>
</span><span style=color:#e6db74>          dbt compile
</span><span style=color:#e6db74>          --project-dir last_successful_main/
</span><span style=color:#e6db74>          --profiles-dir last_successful_main/</span>          
      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Build and test new models</span>
        <span style=color:#f92672>run</span>: &gt;<span style=color:#e6db74>
</span><span style=color:#e6db74>          dbt build
</span><span style=color:#e6db74>          --select state:modified+
</span><span style=color:#e6db74>          --defer
</span><span style=color:#e6db74>          --state last_successful_main/target/</span>          
</code></pre></div><p>Happy coding!</p>Thoughts about the modern data engineering stack<p>Random thoughts about the modern data engineering stack - the modern stack should be the best from the beginning,
or at least aiming to be so.</p><p>It means any required vendor solutions should be onboarded as early as possible,
obviously given the platform vision and financial situation at hand.</p><p>There&rsquo;s no point re-inventing the wheel unless you&rsquo;re trying to solve a unique use-case,
which might be the truth for huge data systems like Netflix or Google has.
Migrations in data world are much more complex comparing it to the regular database migrations in software engineering,
because of the eco-system around all components.
It&rsquo;s hard to isolate and replace a single component.</p><p>Today, when so many products ease the life of a data engineer (managed ETL, data cataloging, governance, etc),
it&rsquo;s already a challenge to pick right and integrate them with each other,
making the job of data analysts and scientists intuitive and easy-going.</p>Reverse ETL - definition and use-cases<p>In this article I&rsquo;d like to summarise my knowledges and understanding of the new buzzword Reverse ETL.</p><p>No long introductions, let&rsquo;s jump into the topic.</p><p>Let&rsquo;s start with definitions, what is ETL and Reverse ETL?</p><p>ETL (Extract-Transform-Load) - a common term for data integration, a process of bringing data from sources to your datawarehouse, or data lake.</p><p><img src=/self/img/2021-11-18-reverse-etl/etl.jpg alt></p><p>Modern data storage and analytics vendors also recommend utilising ELT approach, when transformations and aggregations happen after the load into the datastore.</p><p><img src=/self/img/2021-11-18-reverse-etl/elt.jpg alt></p><p>So what is reverse ETL? As you could guess out of the name - it turns the story the other way around, i.e. it&rsquo;s the process of moving data from the central datastore to operational systems.</p><p><img src=/self/img/2021-11-18-reverse-etl/reverse-etl.jpg alt></p><p>Just google for &ldquo;Reverse ETL&rdquo; to find different vendors pitching their solutions. Most of them would be relatively young, during my research I&rsquo;ve found Hightouch, Census, Grouparoo, Rudderstack, Rivery.</p><p>Use-cases examples for reverse ETL:</p><p>- Identify customer at risk, to prevent customer churn before it happens</p><p>- Drive new sales by matching data from CRM and other sources</p><p>- Data replication to cloud apps for finding new insights and operational analytics</p><p>A regular task for any data engineer is writing a new ETL connector for a new source (or using special tools to ease the process). But having an ETL connector doesn&rsquo;t mean that a reverse ETL connector is also available. In fact, the most common case would be the need to build a reserve ETL connector separately, and to take care about API endpoints, rate limits, etc.</p><p>Real-time scenario requirements:</p><p>In cases when real-time data analysis gives significant benefits (in my personal opinion it&rsquo;s not always the case), it might be complicated to combine it with a reverse ETL - imagine you have hundreds of thousands events per minute, it&rsquo;d be difficult to process all the events through a datawarehouse, designed primarly for batch and complex analytical queries. So in this case it probably makes sense to utilise a regular streaming solutions, like Kafka, optionally with KSQL, and stream necessary data into it, combining in together within the stream itself.</p><p>I personally find the concept of reverse ETL very inspiring, transforming a traditional DWH or data lake into a source of truth across all the systems and 3-rd party providers used inside the company.</p><p>Exciting times we&rsquo;re living in!</p>DWH approaches<p>With this post I&rsquo;d like to cover some well-know approaches to data-warehousing in order to have a brief overview of them:</p><ul><li><a href=#Inmon>Inmon</a></li><li><a href=#Kimball>Kimball</a></li><li><a href=#Data-Vault>Data Vault</a></li><li><a href=#Data-Lake>Data Lake</a></li><li><a href=#Lakehouse>Lakehouse</a></li></ul><p>I&rsquo;m not aiming to cover all the details of each approach in this article, as each of them deserves its own separate article.</p><h3 id=inmon>Inmon</h3><p>Let&rsquo;s start first with the methodology created by Bill Inmon, which is considered to be a top-down approach.
This methodology assumes that the structure of the data is known from top to down, there&rsquo;s a whole picture of it availble
and data is modelled accordingly to the 3NF (Normal form).
Each logical model contains all the details related to that entity. The key point is that data stored in the normalized form.
Data redundancy is avoided as much as possible.
The biggest downside of this approach is that the data should be described from top to down, what required very skillful people
to design it and keep integration between all the components, which might be complicated in the enterprise-level companies.</p><h3 id=kimball>Kimball</h3><p>In opposite to the Inmon approach this one by Ralph Kimball is considered to be a bottom-up approach.
Key sources (operational systems) are identified and documented.<br>Normally it&rsquo;s easier to star twith Kimball, to split the data into datamarts accroding to each department. But it&rsquo;s harder to scale it later.<br>ETL tools bring data from different sources and load it into the staging area.
Then data is loaded into the dimensional tables. The key point is that the data isn&rsquo;t normalized as data is organized
either in Star or Snowflake (under certain circumstances) schema.</p><h3 id=data-vault>Data Vault</h3><p>The DV model, in a nutshell, is a layer that exists between regular dimensional modeling (OLAP, Star Schema) and
Staging that provides scaling with growing business requirements and serves to break down complexities of both the modeling and ETL.
It’s composed of hubs (business entities), links (relationships) and satellites (descriptive attributes)
It&rsquo;s especially useful, when a good support of tracking historical changes is required, as it stores all the facts,
without distinction to good and bad data, is opposite to other DWH methods of storing &ldquo;a single version of truth&rdquo;.</p><p>The modeling method is designed to be resilient to change in the business environment
where the data being stored is coming from, by explicitly separating structural information from descriptive attributes.
Data vault is designed to enable parallel loading as much as possible, so that very large implementations can
scale out without the need for major redesign.</p><h3 id=data-lake>Data lake</h3><p>A very popular approach nowadays, as amount of data grows significantly and introduces silos between different departments,
especially in big enterprise companies.
Opposite to the traditional DWH approaches, Data Lake stores all the information from source systems as-is (i.e. raw data),
optionally some auditing columns to show where the data came from, when it was loaded, etc.
Important to mention it&rsquo;s not just data is use today, but also data that may be used and even data that may never be used just
because it might be used someday.
Hence, commodity, off-the-shelf servers combined with cheap storage makes scaling a data lake to terabytes and petabytes fairly reasonable.
It&rsquo;s like a giant tub of assorted Lego bricks and no defined plan as to how to put it together,
and some of the bricks will be non-standard – it’s up to the person playing with the bricks to assemble it how they see fit to meet their needs.</p><p>Some data lakes have multiple layers e.g. the raw data layer and then a governed data layer where the data has been cleansed,
standardised, etc. - but is still in basically the same structure as in the raw data layer.</p><h3 id=lakehouse>Lakehouse</h3><p>This approach tends to take the best from both worlds: Data Warehouse and Data Lake, providing a single platform
for all data transformations, business intelligence and data science.
Storage and compute levels are separated, implementing similar data structures and data managements features to those in a data darehouse,
directly on the kind of low-cost storage used for data lakes.
The approach assumes the following advantages:</p><ul><li>Data governance may be simplified with a single control point</li><li>Keep data in the same data lake format</li><li>Schema management is simplified</li><li>Transaction support with ACID compliance</li></ul>About the modern data engineering stack<p>Not a secret we&rsquo;re living in the rapidly changing world, generating more and more machine data.
A new era of big data and so on, words you&rsquo;ve probably heard hundreds of times already.
A lot of new and old companies want to get insights out of their data. Become data-driven is a new trend and motto.
Naturally many new, open-source tools and projects appeared against this background.</p><p>How can they help in democratising data landscape and how does the choice between commercial products/services
and open-source/in-house solutions look like in these circumstances?</p><p>I won&rsquo;t give a definitive answer, but rather share my thoughts.</p><p>Every case is pretty unique, although every company can experience similar issues/need of similar instruments.</p><p>Almost every single job description contains some of these keywords: Airflow, Python/SQL, Redshift/Snowflake/BigQuery, Tableau/Looker.</p><p>While it&rsquo;s totally understandable why the latter two are in this list,
I&rsquo;m often curious if it&rsquo;s absolutely necessary to write and maintain a bunch of operators/dags/hooks for Airflow?</p><p>Please get me right, I truly love Airflow, it&rsquo;s a super nice tool,
but still - can data engineers concentrate on more interesting tasks, rather than writing another connector to another source API?
In this light the modern instruments like <a href=https://fivetran.com/>Fivetran</a>/<a href=https://www.getdbt.com/>dbt</a> look quite interesting.</p><p>So here we come to touch the topic of when it makes sense to have a team of data engineers,
maintaining all the DAGs/pipes copying data from sources and custom operators/logic.
It makes sense in the following cases:</p><ul><li>Amount of sources are low, and they aren&rsquo;t frequently added/changed</li><li>Special security concerns</li><li>Specific data sources, which aren&rsquo;t supported by cloud pipelines.</li></ul><p>Every other use-case will potentially win by using <a href=https://www.stitchdata.com/>Stitch</a>/Fivetran/etc - simply because it&rsquo;d be more reliable solution.</p><p>Anyways it&rsquo;s still possible to combine the best from both worlds and use as Airflow tasks, as one of cloud pipelines to ingest the data.
Also there shouldn&rsquo;t be any fear regarding the data engineer&rsquo;s role - no one takes the job position away, but making it easier.
I personally don&rsquo;t want to make myself an important employee, just because I know how a complex piece of some data fetcher&rsquo;s code works.</p><p>In the same time, there&rsquo;s another important factor - team&rsquo;s professional level.
I&rsquo;d agree, that teams having it strong, would potentially benefit more by using their own solutions.
Simply because it&rsquo;d be enough flexible and tuned exactly to fit the company needs.
Quite often such teams also contribute back to the open-source community.</p><p>This is especially actual in the light of the modern data teams concepts - like <a href=https://martinfowler.com/articles/data-monolith-to-mesh.html>Data Mesh</a>,
when the bottle-neck layer of data engineers, responsible for every single data load,
is pushed back to business-teams using this data,
and ideally creating and maintaining ingestion pipelines with the help of data engineers, who support self-service data platform.</p><p>Thank you so much if you read the article all the way here.
I’d love to hear about your opinion and experiences.</p><footer class="footer card-footer text-secondary mt-auto"><div class=row><div class="col col-sm-9">&copy; 2024 Evgenii Karimov</div><div class="col-12 col-sm-3 text-sm-right px-3"><a href=mailto:eskarimov@gmail.com class="text-nowrap text-secondary"><i class="fas fa-envelope fa-fw valign-middle" data-hint=Email></i></a><a href=https://www.linkedin.com/in/evgenii-karimov/ class="text-nowrap text-secondary"><i class="fab fa-linkedin-in fa-fw valign-middle" data-hint=LinkedIn></i></a><a href=https://github.com/eskarimov class="text-nowrap text-secondary"><i class="fab fa-github fa-fw valign-middle" data-hint=Github></i></a><a href=https://medium.com/@evgen-karimov class="text-nowrap text-secondary"><i class="fab fa-medium fa-fw valign-middle" data-hint=Medium></i></a><a href=https://www.instagram.com/eskarimov class="text-nowrap text-secondary"><i class="fab fa-instagram fa-fw valign-middle" data-hint=Instagram></i></a></div></div></footer></body></html>